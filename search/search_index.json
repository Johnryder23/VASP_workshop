{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Intermediate topics for VASP users on NeSI","text":""},{"location":"#learning-goals","title":"Learning goals","text":"<p>Many of you are intermediate to experienced VASP users, and have been using the programme for some time. As VASP evolves and more features become available, however, it is easy to carry on with the same workflows and neglect new release features. It is important we optimise our VASP calculations and make the most  because  VASP users use the most core hours on NeSI, more than any other programme!. In the last year</p> <ul> <li>understand parallelisation options of VASP and how to quickly optimize them for your problem</li> <li>Explore machine learning capability of VASP</li> <li>accelerate your VASP calculations using NeSI's GPUs.</li> <li>........</li> </ul>"},{"location":"#the-basics-vasp-input-files","title":"The basics - VASP input files","text":"<p>VASP generally requires four input files 1. INCAR The runtime settings file for VASP. Tells VASP what type of calculation you want to perform, and the parameters for it to run in. 2. KPOINTS File specifying the k points density. k points are points at which the electronic structure is sampled in the Brillouin zone (reciprocal lattice of a crystalline material). More k points is required for systems with large fluctuations in electron density, as these fluctuations may otherwise be poorly described by corse grain sampling. 3. POTCAR File containing the pseudopotential(s) for all atom(s). Order in which pseudopotentials are given must match the atom order in the <code>POSCAR</code>. 4. POSCAR File specifying atomic coordinates. This file will not be update as the calculation proceeds, instead, updated atomic coordinates are printed to the <code>CONTCAR</code>.</p> <p>Tip</p> <p>We can use ASEs sort utility to sort our <code>POSCAR</code> by atom type, and remove duplicates. This will make writing the <code>POTCAR</code> file much eaiser. <pre><code>#!/usr/bin/env python\n\nfrom ase.io import read\nimport sys\nfrom ase.build import sort\nfrom ase.io import write\n\ninput=read(str(sys.argv[1]))\narrange=sort(input)\nwrite(str(sys.argv[1]),arrange,format=\"vasp\")\n</code></pre></p>"},{"location":"#literature","title":"Literature","text":"<ul> <li>Nieves-P\u00edrez, Isidoro, et al. \"Energy efficiency and performance analysis of a legacy atomic scale materials modeling simulator (VASP).\" The Journal of Supercomputing (2024): 1-24.</li> <li>https://www.nsc.liu.se/~pla/blog/2015/11/16/vaspgpu/</li> </ul>"},{"location":"#workshop-material-structure","title":"Workshop material structure","text":"<p>NOT UP-TO-DATE <pre><code>$ tree\n.\n\u251c\u2500\u2500 ethene_in_vacuum\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 input\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 INCAR\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 KPOINTS\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 POSCAR\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 POTCAR\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 start.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 output\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 CHG\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 CHGCAR\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 CONTCAR\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 DOSCAR\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 EIGENVAL\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 IBZKPT\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 OSZICAR\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 OUTCAR\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 PCDAT\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 PROCAR\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 REPORT\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 vaspout.h5\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 vasprun.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 WAVECAR\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 XDATCAR\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 README.md\n\u251c\u2500\u2500 GPU_calulation\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 input\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 file\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 output\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 file\n\u251c\u2500\u2500 H2O_machine_learning_MD\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 input\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 file\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 output\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 file\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 run.sl\n\u251c\u2500\u2500 INCAR_orig\n\u251c\u2500\u2500 parallel_NEB\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 input\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 file\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 output\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 file\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 very_parallel\n    \u251c\u2500\u2500 input\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 file\n    \u2514\u2500\u2500 output\n        \u2514\u2500\u2500 file\n\n15 directories, 32 files\n</code></pre></p>"},{"location":"GPU_calulation/","title":"GPU calculation","text":""},{"location":"GPU_calulation/#gpu-calculation","title":"GPU calculation","text":"<p>You must use a single MPI rank per GPU. We can make sure the number of GPUs is tied to the number of tasks by using <code>--gpus-per-task</code> instead of <code>--gpus-per-node</code>. Note, on NeSI we can only request a max of 4 GPUs at a time, and consequently, are limited to a max of 4 MPI tasks. Running a VASP job with a max of 4 tasks seems like it would be terribly slow, however, this is not the case because of the following:</p> <ul> <li> <p>When running VASP on GPUs, much less is asked of the CPUs. There are still parts of the code that run CPU-side, but (dependent on the calculation), the GPU(s) now do the heavy lifting.</p> </li> <li> <p>Our GPU versions of VASP support OpenMP multi-threading (I will call these versions MPI+OpenMP VASP, hence the 'hybrid' parallelisation. OpenMP multi-threading is not the same thing as hyperthreading! Please leave <code>--hint=nomultithread</code> on).</p> </li> </ul> <p>Lets expand on the second bullet point above.</p> <p>MPI VASP usually distributes work and data over available MPI ranks on a per-orbital basis (in a round-robin fashion, i.e., Bloch orbital 1 resides on task 1, orbital 2 on task 2. and so on). Since we have so few MPI tasks, we might need to leverage more of the available CPU power, which is where OpenMP threads come in. OpenMP adds a second layer of parallelisation, such that it allows us to distribute a single Bloch orbital over multiple OpenMP threads. These OpenMP threads reside 'within' their parent MPI task.</p> <p>Say we have 2 GPUs and therefore 2 MPI tasks, and 12 OpenMP threads per task. Each OpenMP thread will get its own core, giving us 24 cores. Our Bloch orbitals are assigned to these 24 cores in the same round-robin fashion. You may notice OpenMP effectively does what <code>NCORE</code> did in older versions of VASP. Going forward, OpenMP will be used in place of the <code>NCORE</code> function.</p> <p>Takeaways</p> <p>OpenMP threads are set by --cpus-per-task and assigned to the variable $OMP_NUM_THREADS. The number of physical cores your job will get is equal to --cpus-per-task x --ntasks. The number of cores working on a single orbital will be equal to --cpus-per-task.  </p> <p>Here we have a large supercell, consisting of 232 Si atoms in a Cubic lattice (<code>POSCAR</code> obtained from mp-1201492). That makes a total of 581 KS orbitals!</p> <p>If we were to perform a typical electronic minimization on this structure using the following <code>INCAR</code>: <pre><code>#Electronic relaxation\n ENCUT = 300              #specifies the cutoff energy in eV for the planewave basis set\n EDIFF = 1E-4             #electronic break condition\n\n#Ionic relaxation\n EDIFFG = -0.001          #ionic relaxation break condition. If negative, the relaxation is stopped when the norms of all the forces are smaller than |EDIFFG|.\n IBRION = 1               #determines how the ions are updated and moved\n ISYM = 0                 #symmetry treatment\n\n#Method\n GGA = PE \u00a0               #specifies the exchange-coorelation functional used\n</code></pre> And the following compute resources: <pre><code>#SBATCH --nodes=1\n#SBATCH --partition=milan\n#SBATCH --ntasks=128\n#SBATCH --mem=50G\n#SBATCH --hint=nomultithread\n</code></pre> i.e., an entire node on the Milan partition - this calculation takes 01:14:40 to finish.</p> <p>Note</p> <p>In this calculation only a single ionic step is performed as norms of all the forces between atoms are less than 0.001 eV.</p> <p>We can easily see that if this <code>POSCAR</code> was more dissorded and a few dozen ionic steps were required, this calculation would easily take more than a day to complete.</p> <p>Now try this same example on a single P100 GPU. A few changes are required in the <code>INCAR</code>: <pre><code>#Electronic relaxation\n ENCUT = 300              #specifies the cutoff energy in eV for the planewave basis set\n EDIFF = 1E-4             #electronic break condition\n\n#Ionic relaxation\n EDIFFG = -0.001          #ionic relaxation break condition. If negative, the relaxation is stopped when the norms of all the forces are smaller than |EDIFFG|.\n IBRION = 1               #determines how the ions are updated and moved\n ISYM = 0                 #symmetry treatment\n\n#Method\n GGA = PE \u00a0               #specifies the exchange-coorelation functional used\n\n#Parallelization\n#NCORE =            #determines the number of compute cores that work on an individual orbital\nKPAR = 2            #determines the number of k-points that are to be treated in parallel\nNSIM = 10           #sets the number of bands that are optimized simultaneously by the RMM-DIIS algorithm (ALGO=Normal)\n</code></pre> Do not set <code>NCORE</code> for a VASP GPU job. Anything different from <code>NCORE=1</code> will significantly slow down a GPU run. This setting is outdate anyway,  OpenMP </p> <p>Also, our Slurm submission script must be adapted: <pre><code>&lt;print slurm script here&gt;\n</code></pre> text text text discuss batch script changes</p> <p>\"Theoretically, the GPU calculations will run faster the higher the value of NSIM you set, with the drawback being that the memory consumption on the GPUs increase with higher NSIM as well. The recommendation from the developers is that you should increase NSIM as much you can until you run out of memory. This can require some experimentation, where you have to launch your VASP job and the follow the memory use with the nvidia-smi -l command. I generally had to stop at NSIM values of 16-32.\" From <code>Running VASP on Nvidia GPUs</code></p> <p>Note</p> <p>the message <code>Warning: ieee_invalid is signaling</code> may appear in your stdout file. This is only a warning, and can safely ignored. We can mute this warning, however, by setting <code>export NO_STOP_MESSAGE=1</code> in our submit script.</p> <p>There is still some work that needs to be done on the GPU. now all </p> <p>So lets start this job with 2 NVDIA Tesla P100 GPUs.</p> <p>notice in the INCAR we set EDIFFG = EDIFF = 0. so we will perform 10 electronic SCF loops and 5 ionic steps.</p> <p>Lets go onto the node and monitor the GPUs in real time with the <code>nvidia-smi</code> command line utility.</p> why is the PID from <code>nvidia-smi</code> different to that printed in output file? <p>notice that the PIDs listed by 'nvidia-smi' are different than those listed at the top of the job_*.out file. Why is this? Recall we started VASP with the following <code>srun</code> command: <pre><code>srun bash -c 'echo -e \"\\nI am MPI task #${SLURM_PROCID}. Since there are ${OMP_NUM_THREADS} OpenMP threads, I will be assigned the following ${OMP_NUM_THREADS} physical cores on $(hostname):\\n$(taskset -apc $$)\\nRecall, MPI VASP distributes work and data over the MPI ranks on a per-orbital basis (in a round-robin fashion). MPI+OpenMP VASP, however, distributes the work of each Bloch orbital over the ${OMP_NUM_THREADS} OpenMP threads, and not between MPI tasks.\" &amp;&amp; vasp_std' \n</code></pre> where <code>taskset -apc $$</code> returned the process ID of the current shell instance. If we go onto the compute node for this job and run <code>pstree -p &lt;PID_from_job_*.out&gt;</code> we will see that <code>vasp_std</code> (the one running on the GPU) is a child process of the Slurm script shell PID.</p> <p>How much faster is this with a GPU than it is on CPUs?</p> <p>Lets adjust our submit script slightly to run this without a GPU.</p>"},{"location":"background/","title":"Background","text":"<p>recomemded reading for this section:</p> <ul> <li>Cramer, C. J. Essentials of Computational Chemistry: Theories and Models; John Wiley &amp; Sons, 2004; Chapter 8.</li> </ul>"},{"location":"background/#implementation-of-dft-in-vasp","title":"Implementation of DFT in VASP","text":"<p>VASP can perform methods other than DFT, but here we focus only on the DFT Implementation.</p> <p>The Hohenberg\u2013Kohn Existence Theorem and Hohenberg\u2013Kohn Variational Theorem proved that it is possible to construct the Hamiltonian, and in turn the wavefunction, solely based on electron density, \\(n(r)\\). Constructing the Hamiltonian in terms of density does not alleviate the Schr\u00f6dinger equations centrosymmetry issue, though, as the electron-electron interaction has simply been redifned. So a further condition is required. The additional condition is to approximate our system of interest as a system of non-interacting electrons with the same overall density as the real system (this set of non-interacting electrons are the KS orbitals). KS orbitals are iteratively solved, returning lower energies until \\(n(r)KS=n(r)\\) (Variational Theorem). Using this method the real density (\\(n(r)\\)) can been determined from one electron density functions without approximation. In VASP, we determine the calculation is complete when differences in \\(E[n(r)KS]\\) are within <code>EDIFF</code>, or in other words, when \\(E[n(r)KS]\\) is within <code>EDIFF</code> of \\(E[n(r)]\\).</p> <p>Note</p> <p>Approximations do eventually enter KS-DFT. Namely the correction to the kinetic energy deriving from the interacting nature of the electrons, and all non-classical corrections to the electron\u2013electron repulsion energy. The exchange-correlation functional (\\(E_{XC}\\)) deals with these terms, which we choose with the <code>GGA</code> tag in the <code>INCAR</code>.</p> <p>VASPs main task is to solve the Kohn-Sham (KS) one electron orbitals of our system according to the eigenvalue equation below:</p> \\[\\begin{equation} H^{ks}\\psi_n(r)=\\epsilon_n \\psi_n(r) \\tag{a} \\label{KS_energy} \\end{equation}\\] <p>Where \\(H^{ks}\\) is the effective Hamiltonian and \\(\\psi_n(r)\\) and \\(\\epsilon_n\\) are the wavefunction (eigenfunction) and energy (eigenvalue) of KS orbital \\(n\\). The final KS wavefunction (printed in the <code>WAVECAR</code> when VASP finishs) is a single Slater determinant made of the set of orbitals that are the lowest-energy solutions to Equation \\ref{KS_energy}.</p> <p>Because VASP is often used for bulk-like materials,         The projector-augmented-wave (PAW) methos is used </p>"},{"location":"hybrid_parallelisation/","title":"Hybrid parallelisation, Combining MPI and OpenMP","text":"<p>See Combining MPI and OpenMP for this section</p> Parallelisation terminology <p>The power of HPC comes from its scalability. Scientific programmes on a HPC need to make use of some form of parallelisation to exploit this scalability. Parallelisation methods are broadly broken into two catagories - shared memory parallelisation and distributed memory parallelisation. As the names imply, shared memory parallelisation requires shared RAM between CPUs, where distributed memory parallelisation can run across CPUs that do not share RAM. This means shared memory parallelisation must use CPUs on the same node, whereas distributed memory parallelisation can use CPUs on any number of nodes. OpenMP (Open Multi-Processing) and MPI (Message Passing Interface) are common implimentations of shared memory parallelisation and distributed memory parallelisation respectively. </p>"},{"location":"hybrid_parallelisation/#why-would-we-want-to-use-mpi-openmp-parallelisation","title":"Why would we want to use MPI + OpenMP parallelisation?","text":"<p>On NeSI, VASP is available with OpenMP for all versions at or above VASP6.x.x.</p> <p>The addition of OpenMP parallelisation is fairly new to VASP. What was wrong with just using MPI? Why do we need OpenMP </p>"},{"location":"hybrid_parallelisation/#how-to-use-mpi-openmp-parallelisation","title":"How to use MPI + OpenMP parallelisation","text":""},{"location":"interactive_job/","title":"Interactive Job","text":""},{"location":"interactive_job/#general","title":"General","text":"<p>This calculation is a standard ionic and electronic energy minimisation for ethane in vacuum. Because the calculation is short, it can be run interactively from the command line. A Bash script will be used to set up and submit the calculation.</p> <p>To start an interactive Slurm job, we will execute <code>srun</code> directly - and not from within a batch script sumitted with <code>sbatch</code>. Subbmitting with <code>srun</code> instead of <code>sbatch</code> makes the job execute in real time, or, in other words, reads <code>stdin</code> from the terminal and prints 'stdout' and 'stderr' to the terminal. </p> <p>Warning</p> <p>running a job in this way means if your connection with NeSI drops, the job will fail - as 'stdin' is broken. Therefore, running jobs interactively is only appropriate for short calculations or quick checks.</p>"},{"location":"interactive_job/#input-parameters","title":"Input parameters","text":"<p>Lets start by inspecting the <code>./input/INCAR</code> file.</p> <p>We set the desired exchange-correlation functional with the <code>GGA</code> tag. Keep in mind the GGA type should match that of the <code>POTCAR</code> pseudopotential. If these do not match, VASP will give a <code>warning</code>.</p> <p>DISCUSS parallelisation options used in this calc NPAR HERE</p> <p>I happen to know that this calculation has 12 bands (i.e., <code>NBANDS</code>= 12) ...describe how we find NBANDS before running full calc here... Knowing this, lets keep it simple and submit our calculation with 12 cores, such that each band (or KS orbitals) gets its own core.</p> <p>We do this by setting <code>NPAR = 12</code> in the <code>INCAR</code>.</p>"},{"location":"interactive_job/#start-the-calculation","title":"Start the calculation","text":"<p>Inspect and then execute the script <code>start.sh</code> (by typing <code>./input/start.sh</code>). You may also wish to inspect the other files in the <code>input</code> directory.</p>"},{"location":"interactive_job/#approximate-runtime-statistics","title":"Approximate runtime statistics","text":""},{"location":"interactive_job/#total-cpu-time-approx-xxxxxx","title":"total cpu time (approx): XX:XX:XX","text":"<p>memory used: <code>XXX</code></p> <p>MPI tasks: <code>XXX</code></p> <p>CPU efficiency: <code>XXX</code></p> <p>OpenMP threads: <code>XXX</code></p>"}]}